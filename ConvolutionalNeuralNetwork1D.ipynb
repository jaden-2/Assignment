{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple1DConv:\n",
    "    def __init__(self, f_size, initializer, optimizer, p =1, stride=1, n_channel_in = 1, n_channel_out= 1):\n",
    "        self.init = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.n_channel_in = n_channel_in\n",
    "        self.n_channel_out = n_channel_out\n",
    "        self.stride = stride\n",
    "        self.pad = p\n",
    "        self.n_out = None\n",
    "        self.f_size = f_size\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.n_in = X.reshape(-1, 1).shape[0] # number of features\n",
    "        self.n_samples = X.reshape(-1, 1).shape[1] # number of samples\n",
    "\n",
    "        self.x_new = X.reshape(self.n_channel_in, self.n_samples, self.n_in) # reshape into 3D array of (layers, number of samples, number of features)\n",
    "        self.x_new = np.pad(self.x_new, ((0, 0), (0, 0), (self.pad, self.pad))) # Adds padding around row and column for each layer\n",
    "        \n",
    "        self.n_out = num_out(self.n_in, self.pad, self.f_size, self.stride) # calculates the number of output from this convolutional layer\n",
    "        \n",
    "        output = np.zeros((self.n_channel_in, self.n_samples, self.n_out)) # features\n",
    "\n",
    "        # initialization of weights and biases\n",
    "        self.w = self.init.W((self.n_channel_in, self.n_samples, self.f_size))\n",
    "        self.b = self.init.B((self.n_channel_in, self.n_samples, self.f_size))\n",
    "\n",
    "        # Loops through each layer, convolutes the rows and columns for each, and performs forward propagation\n",
    "        for i in range(self.n_channel_in):\n",
    "            a = np.array([]) # features extracted by filter\n",
    "            for j in range(0, self.x_new.shape[-1], self.stride):\n",
    "                if j + self.f_size > self.x_new.shape[-1]:\n",
    "                    break\n",
    "                a = np.append(a, np.sum(self.x_new[i][:, j: j + self.f_size][0] @ self.w[i][0] + self.b[i][0]))\n",
    "     \n",
    "            output[i] = a\n",
    "            #a = np.array(a).reshape(1, 1, a.shape[-1]) # reshape into (1, 1, number of features)\n",
    "            #output = np.append(output, a, axis=0) # appended extracted features to layer\n",
    "\n",
    "        return output.sum(axis=0)\n",
    "\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        self.db = np.sum(dA, axis=(0, -1))\n",
    "        n_in = self.x_new.shape[-1] # number of input features\n",
    "        self.dw = np.zeros((self.n_channel_in, self.n_samples, self.f_size))\n",
    "        l = dA.shape[-1]\n",
    "        for i in range(self.n_channel_in):\n",
    "            layer = self.x_new[i]\n",
    "            w = np.array([])\n",
    "            for j in range(0, n_in, self.stride):\n",
    "                if j + l > n_in:\n",
    "                    break           \n",
    "                w = np.append(w, dA[i] @ layer[:, j: j + l][0])        \n",
    "            self.dw[i] = w\n",
    "\n",
    "        self.dx = np.zeros((self.n_channel_in, 1, self.f_size))\n",
    "\n",
    "        a = np.pad(dA, ((0, 0), (0, 0), (self.stride, self.stride)))\n",
    "        dx = np.ones((self.n_channel_in, dA.shape[1], self.n_out))\n",
    "        for  i in range(self.n_channel_in):\n",
    "            layer = a[i]\n",
    "            weight = self.w[i]\n",
    "            w = np.array([])\n",
    "            for j in range(0, a.shape[-1], self.stride):\n",
    "                if j + self.f_size > a.shape[-1]:\n",
    "                    break\n",
    "                w = np.append(w, layer[:, j: j+self.f_size][0] @ weight[0])\n",
    "            dx[i] = w\n",
    "        \n",
    "        self.optimizer(self)\n",
    "        return dx\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def num_out(n_in, pad, f, s):\n",
    "    \"\"\"\n",
    "    ##### self.n_in: number of features\n",
    "    ##### pad: Number of padding on one side\n",
    "    ##### s: stride value\n",
    "\n",
    "    ##### Returns: Number of output of convolution\n",
    "    \"\"\"\n",
    "    n_out = ((n_in + 2*pad - f)/ s) +1\n",
    "    return int(n_out)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      Number of nodes in the previous layer\n",
    "    n_nodes2 : int\n",
    "      Number of nodes in the later layer\n",
    "    initializer: instance of initialization method\n",
    "    optimizer: instance of optimization method\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.weights = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.bias = initializer.B(n_nodes2)\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
    "            output\n",
    "        \"\"\"        \n",
    "        self.Z = X\n",
    "        A = X @ self.weights + self.bias\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient flowing from behind\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
    "            Gradient to flow forward\n",
    "        \"\"\"\n",
    "        \n",
    "        # update\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.dW = self.Z.T @ dA \n",
    "        self.dZ = dA @ self.weights.T\n",
    "        \n",
    "        self.optimizer.update(self)\n",
    "        \n",
    "        return self.dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization with Gaussian distribution\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, shape):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        Shape: tuple\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        w = self.sigma * np.random.randn(*shape)\n",
    "        \n",
    "        return w\n",
    "    \n",
    "    def B(self, shape):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple\n",
    "          Number of nodes in the later layer\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(*shape)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.A = X\n",
    "        return np.clip(X, 0, None)\n",
    "    \n",
    "    def backward(self, X):\n",
    "        a = X > 0\n",
    "        return X * np.clip(np.sign(self.A), 0, None)\n",
    "\n",
    "\n",
    "class Softmax():\n",
    "        def __init__(self) -> None:\n",
    "            pass\n",
    "\n",
    "\n",
    "        def forward(self, a):\n",
    "            numerator = np.exp(a)\n",
    "            self.Z = numerator / np.sum(np.exp(a), axis=1).reshape(-1, 1)\n",
    "            return self.dZ\n",
    "        \n",
    "        def backward(self, Y):\n",
    "             self.loss = self.loss_func(Y)\n",
    "\n",
    "             return self.dZ - Y\n",
    "        \n",
    "        def loss_func(self, Y, Z = None):             \n",
    "            if Z == None:\n",
    "                Z = self.Z\n",
    "            return (-1) * np.average(np.sum(Y * np.log(Z)), axis=1)\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrab:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr \n",
    "        self.ssg = 0\n",
    "\n",
    "    def update(self, layer):\n",
    "        \n",
    "        self.ssg += np.sum(layer.weights ** 2)  \n",
    "        adaptive_lr = self.lr / np.sqrt(self.ssg + 1e-8)\n",
    "        layer.weights -= (self.lr - adaptive_lr) * (layer.weights)\n",
    "        layer.bias -= (self.lr - adaptive_lr) * layer.bias\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.03419906, 0.06430012, 0.09440119, 0.12450226, 0.15460332,\n",
       "         0.18470439, 0.21480545, 0.24490652, 0.12432047]]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Simple1DConv(3, SimpleInitializer(0.05), AdaGrab(0.01))\n",
    "\n",
    "input = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "model.forward(input)\n",
    "\n",
    "\n",
    "input = input.reshape((1, 1, input.shape[-1]))\n",
    "\n",
    "model.backward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7]\n",
      "[1 2 3 4 5 6 7 8]\n",
      "[2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "input = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "input = np.pad(input, (1, 1))\n",
    "print(input[0:8])\n",
    "print(input[1:9])\n",
    "print(input[2:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "Iterator to get a mini-batch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "      Training data\n",
    "    y : The following form of ndarray, shape (n_samples, 1)\n",
    "      Correct answer value\n",
    "    batch_size : int\n",
    "      Batch size\n",
    "    seed : int\n",
    "      NumPy random seed\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int_)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
