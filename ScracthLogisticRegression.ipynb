{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    Scratch implementation of logistic regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      Number of iterations\n",
    "    lr : float\n",
    "      Learning rate\n",
    "    no_bias : bool\n",
    "      True if no bias term is included\n",
    "    verbose : bool\n",
    "      True to output the learning process\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : The following form of ndarray, shape (n_features,)\n",
    "      Parameters\n",
    "    self.loss : The following form of ndarray, shape (self.iter,)\n",
    "      Record losses on training data\n",
    "    self.val_loss : The following form of ndarray, shape (self.iter,)\n",
    "      Record loss on validation data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_iter = 100, lr = 1e-5, bias=False, verbose=False, lamb=0.1):\n",
    "        # Record hyperparameters as attributes\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        # Prepare an array to record the loss\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        # parameter\n",
    "        self.theta = np.array([])\n",
    "        self.lamb = lamb\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Learn logistic regression. If validation data is entered, the loss and accuracy for it are also calculated for each iteration.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of training data\n",
    "        y : The following form of ndarray, shape (n_samples,)\n",
    "            Correct answer value of training data\n",
    "        X_val : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of verification data\n",
    "        y_val : The following form of ndarray, shape (n_samples,)\n",
    "            Correct value of verification data\n",
    "        \"\"\"\n",
    "        if self.bias == True:\n",
    "            a = np.ones(X.shape[0]).reshape((-1, 1))\n",
    "            X = np.hstack([a, X])\n",
    "\n",
    "        self.theta = np.zeros(X.shape[1]).reshape((-1, 1))\n",
    "        for i in range(self.iter):\n",
    "            self._gradient_descent(X, y)\n",
    "            y_pred = self._logistic_hypothesis(X)\n",
    "            self.loss = np.append(self.loss, self._loss_func(y_pred, y))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Estimate the label using logistic regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            The following form of ndarray, shape (n_samples, 1)\n",
    "            Estimated result by logistic regression\n",
    "        \"\"\"\n",
    "        if self.bias == True:\n",
    "            a = np.ones(X.shape[0]).reshape((-1, 1))\n",
    "            X = np.hstack([a, X])\n",
    "\n",
    "\n",
    "        return np.where(self._logistic_hypothesis(X) >=0.5, 1, 0)\n",
    "\n",
    "       \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Estimate the probability using logistic regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            The following form of ndarray, shape (n_samples, 1)\n",
    "            Estimated result by logistic regression\n",
    "        \"\"\"\n",
    "        if self.bias == True:\n",
    "            a = np.ones(X.shape[0]).reshape((-1, 1))\n",
    "            X = np.hstack([a, X])\n",
    "        return self._logistic_hypothesis(X)\n",
    "    \n",
    "    def hypothesis(self, x):\n",
    "        return x @ self.theta\n",
    "    \n",
    "    def _logistic_hypothesis(self, x):\n",
    "        y_pred = self.hypothesis(x)\n",
    "        return 1/ (1 + np.exp(-y_pred))\n",
    "    \n",
    "    # gradient descent is buggy\n",
    "    def _gradient_descent(self, x, y):\n",
    "        rows = x.shape[0]\n",
    "        columns = x.shape[1]\n",
    "        y_pred = self._logistic_hypothesis(x)\n",
    "\n",
    "        for column in range(columns):\n",
    "\n",
    "            gradient = 0\n",
    "            for row in range(rows):\n",
    "                gradient += (y_pred[row] - y[row]) * x[row, columns]\n",
    "            self.theta[column] = self.theta[column] - self.lr * ( gradient + self.lamb*self.theta[column])/rows\n",
    "            \n",
    "    \n",
    "    def _loss_func(self, pred, y):\n",
    "        error = 0\n",
    "        for i in range(y.shape[0]):\n",
    "            error += -np.sum(y[i] *  np.log(pred[i])+(1-y[i]) *  np.log(1-pred[i]))\n",
    "        loss = error / (y.shape[0])\n",
    "        loss = loss + np.sum(self.theta**2)*self.lamb/(2 * y.shape[0])\n",
    "        return loss\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "features = iris.data\n",
    "target = iris.target \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.51526936, -0.78879272,  2.16216075,  0.90876014]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(X))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression().fit(x_train, y_train)\n",
    "\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([-0.50863213,  0.75978407, -2.38787951, -0.94154691]).reshape((-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98404322],\n",
       "       [0.99999801],\n",
       "       [0.99999958],\n",
       "       [0.96892367],\n",
       "       [0.99992443],\n",
       "       [0.98624802],\n",
       "       [0.98458867],\n",
       "       [0.976894  ],\n",
       "       [0.98620355],\n",
       "       [0.99993343],\n",
       "       [0.99999891],\n",
       "       [0.99999605],\n",
       "       [0.99024753],\n",
       "       [0.9760031 ],\n",
       "       [0.97264521],\n",
       "       [0.98361745],\n",
       "       [0.96865292],\n",
       "       [0.95095343],\n",
       "       [0.98029721],\n",
       "       [0.99999701]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x_test @ a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd \n",
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "data = pd.concat([iris.data, iris.target],axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"target\"] != 2]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.iloc[:, :-1] , data.iloc[:, -1],test_size=0.2)\n",
    "x_train, x_test, y_train, y_test = x_train.values, x_test.values, y_train.values, y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ScratchLogisticRegression(lr=1e-3).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45263746],\n",
       "       [0.5010632 ],\n",
       "       [0.50046433],\n",
       "       [0.44644599],\n",
       "       [0.44864311],\n",
       "       [0.44446812],\n",
       "       [0.50851554],\n",
       "       [0.44692253],\n",
       "       [0.4520566 ],\n",
       "       [0.44717246],\n",
       "       [0.45737193],\n",
       "       [0.45376713],\n",
       "       [0.49949356],\n",
       "       [0.44663794],\n",
       "       [0.50696824],\n",
       "       [0.44729718],\n",
       "       [0.49809433],\n",
       "       [0.50014386],\n",
       "       [0.50497188],\n",
       "       [0.45656845]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x_test @ a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
