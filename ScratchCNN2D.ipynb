{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    def __init__(self, n_filter, f_size, initializer, optimizer, activation, p =1, stride=1, n_channel_in = 1, n_channel_out= 1, debug = False):\n",
    "        self.init = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.n_channel_in = n_channel_in\n",
    "        self.n_channel_out = n_channel_out\n",
    "        self.stride = stride\n",
    "        self.pad = p\n",
    "        self.n_out = None\n",
    "        self.f_size = f_size\n",
    "        self.activate = activation\n",
    "        self.n_filter = n_filter\n",
    "        self.debug = debug\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.n_samples, im_rows, im_col = X.shape\n",
    "        \n",
    "        channels = self.n_channel_in\n",
    "        X = X.reshape(self.n_samples, self.n_channel_in, im_rows, im_col)\n",
    "        \n",
    "        self.X = X\n",
    "        # initialization of weights and biases\n",
    "        self.w = self.init.W((self.n_channel_in, self.f_size, self.f_size))\n",
    "        self.b = self.init.B((self.n_channel_in, 1))\n",
    "        \n",
    "        self.n_out_row = num_out(im_rows, self.pad, self.f_size, self.stride)\n",
    "        self.n_out_col = num_out(im_col, self.pad, self.f_size, self.stride)\n",
    "\n",
    "        result = np.zeros((self.n_samples, self.n_channel_in, self.n_out_row, self.n_out_col), dtype=np.float64)\n",
    "\n",
    "        # padding of samples and kernel\n",
    "        nx = im_rows + self.f_size - 1\n",
    "        ny = im_col + self.f_size - 1\n",
    "\n",
    "        self.x_new = np.pad(X, ((0, 0), (0, 0), (0, nx - im_rows), (0, ny - im_col)), mode=\"constant\")\n",
    "        self.w_new = np.pad(self.w, ((0, 0), (0, nx - self.f_size), (0, ny - self.f_size)))\n",
    "\n",
    "        # loops through samples of images\n",
    "\n",
    "        \"\"\"convolution theorem states that under suitable conditions \n",
    "        the Fourier transform of a convolution of two functions (or signals) \n",
    "        is the pointwise product of their Fourier transforms.\"\"\"\n",
    "\n",
    "        for i in range(self.n_samples):\n",
    "            sample = self.x_new[i]\n",
    "\n",
    "            for ch in range(channels):\n",
    "                channel = np.fft.fft2(sample[ch])\n",
    "                weight = np.fft.fft2(self.w_new[ch])\n",
    "                s = self.f_size - 2\n",
    "                out = np.fft.ifft2(channel * weight)\n",
    "                out = np.real(out)[s:-s:self.stride, s:-s:self.stride] + self.b[ch]\n",
    "                \n",
    "                result[i, ch] = out\n",
    "        dA = result.sum(1)\n",
    "        \n",
    "        if self.debug:\n",
    "            return dA\n",
    "        \n",
    "        return self.activate.forward(dA)\n",
    "\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \n",
    "        if not self.debug:\n",
    "            dA = self.activate.backward(dA)\n",
    "            \n",
    "        dA = self.activate.backward(dA)\n",
    "        self.dB = np.sum(dA, dtype=np.float64)\n",
    "\n",
    "        dA = dA.reshape(self.X.shape)\n",
    "        rows_da = dA.shape[-2]\n",
    "        cols_da = dA.shape[-1]\n",
    "        channels = dA.shape[1]\n",
    "        n_samples, _, n_rows, n_cols = self.x_new.shape \n",
    "\n",
    "        nx = rows_da + n_rows - 1\n",
    "        ny = cols_da + n_rows -1 \n",
    "\n",
    "        dA_pad = np.pad(dA, ((0, 0), (0, 0), (0, nx - rows_da), (0, ny - cols_da)))\n",
    "        x_pad = np.pad(self.x_new, ((0, 0), (0, 0), (0, nx - n_rows), (0, ny - n_cols)))\n",
    "\n",
    "        d_x = rows_da - 1\n",
    "        d_y = cols_da - 1\n",
    "        w = np.ones((n_samples, channels, self.f_size, self.f_size))\n",
    "        for i in range(n_samples):\n",
    "            sample_da =  dA_pad[i]\n",
    "            sample_x = x_pad[i]\n",
    "\n",
    "            for ch in range(channels):\n",
    "                layer_da = np.fft.fft2(sample_da[ch])\n",
    "                layer_x = np.fft.fft2(sample_x[ch])\n",
    "\n",
    "                c = np.fft.ifft2(layer_da * layer_x)\n",
    "                c = np.real(c)[d_x:-d_x: self.stride, d_y: -d_y:self.stride]\n",
    "                \n",
    "                w[i, ch] = c\n",
    "        \n",
    "        self.dW = w.sum(0)\n",
    "\n",
    "        dx = np.zeros((n_samples, channels, self.n_out_row, self.n_out_col))\n",
    "\n",
    "        weights = self.w\n",
    "\n",
    "        nx = rows_da + self.f_size - 1\n",
    "        ny = cols_da + self.f_size - 1\n",
    "\n",
    "        dA_pad = np.pad(dA, ((0, 0), (0, 0), (0, nx - rows_da), (0, ny - cols_da)), mode=\"constant\")\n",
    "        weights = np.pad(weights,((0, 0), (0, nx - self.f_size), (0, ny - self.f_size)), mode=\"constant\")\n",
    "        s = self.f_size - 2\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            sample = dA_pad[i]\n",
    "            for ch in range(channels):\n",
    "                a = np.fft.fft2(sample[ch])\n",
    "                b = np.fft.fft2(weights[ch])\n",
    "                c = np.real(np.fft.ifft2(a*b))[s:-s:self.stride, s:-s:self.stride]\n",
    "                \n",
    "                dx[i, ch] = c\n",
    "        \n",
    "        self.optimizer.update(self)\n",
    "\n",
    "        return dx\n",
    "        \n",
    "\n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "def num_out(n_in, pad, f, s):\n",
    "    \"\"\"\n",
    "    ##### self.n_in: number of features\n",
    "    ##### pad: Number of padding on one side\n",
    "    ##### s: stride value\n",
    "\n",
    "    ##### Returns: Number of output of convolution\n",
    "    \"\"\"\n",
    "    n_out = ((n_in + 2*pad - f)/ s) +1\n",
    "    return int(n_out)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D():\n",
    "    def __init__(self, P):\n",
    "        self.P = P\n",
    "        self.PA = None\n",
    "        self.Pindex = None\n",
    "    \n",
    "    def forward(self,A):\n",
    "        N,OH,OW = A.shape\n",
    "        F = 1\n",
    "        PH,PW = int(OH/self.P),int(OW/self.P)\n",
    "        self.params = N,F,OH,OW,self.P,PH,PW\n",
    "        self.PA = np.zeros([N,PH,PW])\n",
    "        self.Pindex = np.zeros([N,PH,PW])\n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(PH):\n",
    "                    for col in range(PW):\n",
    "                        #print(np.max(A[n,ch,row*self.P:row*self.P+self.P,col*self.P:col*self.P+self.P]).shape)\n",
    "                        self.PA[n,row,col] = np.max(A[n,row*self.P:row*self.P+self.P,col*self.P:col*self.P+self.P])\n",
    "                        \n",
    "                        self.Pindex[n,row,col] = np.argmax(A[n, row*self.P:row*self.P+self.P,col*self.P:col*self.P+self.P])\n",
    "        \n",
    "        return self.PA\n",
    "    \n",
    "    def backward(self,dA):\n",
    "        N,F,OH,OW,PS,PH,PW = self.params\n",
    "        dP = np.zeros([N,F,OH,OW])\n",
    "        for n in range(N): \n",
    "            for ch in range(F):\n",
    "                for row in range(PH):\n",
    "                    for col in range(PW):\n",
    "                        idx = self.Pindex[n,ch,row,col]\n",
    "                        tmp = np.zeros((PS*PS))\n",
    "                        for i in range(PS*PS):\n",
    "                            if i == idx:\n",
    "                                tmp[i] = dA[n,ch,row,col]\n",
    "                            else:\n",
    "                                tmp[i] = 0\n",
    "                        dP[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS] = tmp.reshape(PS,PS)\n",
    "        return dP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, X):\n",
    "        self.shape = X.shape\n",
    "        return X.reshape(self.shape[0], -1)\n",
    "    \n",
    "    def backward(self,X):\n",
    "        return X.reshape(self.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      Number of nodes in the previous layer\n",
    "    n_nodes2 : int\n",
    "      Number of nodes in the later layer\n",
    "    initializer: instance of initialization method\n",
    "    optimizer: instance of optimization method\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, activator):\n",
    "        self.optimizer = optimizer\n",
    "        self.w = initializer.W((n_nodes1, n_nodes2))\n",
    "        self.b = initializer.B((n_nodes2,))\n",
    "        self.activator = activator\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
    "            output\n",
    "        \"\"\"        \n",
    "        self.Z = X\n",
    "        A = X @ self.w + self.b\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient flowing from behind\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
    "            Gradient to flow forward\n",
    "        \"\"\"\n",
    "        \n",
    "        # update\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "\n",
    "       # print(self.dB)\n",
    "        self.dW = self.Z.T @ dA \n",
    "        self.dZ = dA @ self.w.T\n",
    "        \n",
    "        self.optimizer.update(self)\n",
    "        \n",
    "        return self.dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization with Gaussian distribution\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, shape):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        Shape: tuple\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        w = self.sigma * np.random.randn(*shape)\n",
    "        \n",
    "        return w\n",
    "    \n",
    "    def B(self, shape):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple\n",
    "          Number of nodes in the later layer\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(*shape)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr \n",
    "        self.HW = 1\n",
    "        self.HB = 1\n",
    "\n",
    "    def update(self, layer):\n",
    "\n",
    "        self.HW += layer.dW**2\n",
    "        self.HB += layer.dB**2\n",
    "\n",
    "        layer.w -= self.lr * np.sqrt(1/self.HW) * (layer.dW)\n",
    "        layer.b -= self.lr * np.sqrt(1/self.HB) *  layer.dB\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : Learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        The weight or bias of a certain layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance of the layer before update\n",
    "        \"\"\"\n",
    "\n",
    "        layer.w -= self.lr * layer.dW\n",
    "        layer.b -= self.lr * layer.dB\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.A = X\n",
    "        return np.clip(X, 0, None)\n",
    "    \n",
    "    def backward(self, X):\n",
    "        a = X > 0\n",
    "        return X * np.clip(np.sign(self.A), 0, None)\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.tanh(A)\n",
    "    def backward(self, dZ):\n",
    "        return dZ * (1 - (np.tanh(self.A))**2)\n",
    "\n",
    "class Softmax():\n",
    "        def __init__(self) -> None:\n",
    "            pass\n",
    "\n",
    "\n",
    "        def forward(self, a):\n",
    "            numerator = np.exp(a)\n",
    "            self.dZ = numerator / np.sum(np.exp(a), axis=1).reshape(-1, 1)\n",
    "            return self.dZ\n",
    "        \n",
    "        def backward(self, Y):\n",
    "             self.loss = self.loss_func(Y)   \n",
    "                      \n",
    "             return self.dZ - Y\n",
    "        \n",
    "        def loss_func(self, Y, Z = None):             \n",
    "            if type(Z) == type(None):\n",
    "                Z = self.dZ\n",
    "            \n",
    "            loss = -1* np.sum(Y * np.log(Z + 1e-7))\n",
    "\n",
    "            return loss/len(Y)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "Iterator to get a mini-batch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "      Training data\n",
    "    y : The following form of ndarray, shape (n_samples, 1)\n",
    "      Correct answer value\n",
    "    batch_size : int\n",
    "      Batch size\n",
    "    seed : int\n",
    "      NumPy random seed\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int_)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ScratchConvolutionalNeuralNetwork2D:\n",
    "    def __init__(self, NN, CNN, epoch=20, batch_size=20, verbose = False):\n",
    "        self.epoch = epoch\n",
    "        self.NN = NN\n",
    "        self.CNN = CNN\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.log_loss = np.zeros(self.batch_size)\n",
    "        self.log_acc = np.zeros(self.batch_size)\n",
    "\n",
    "        \n",
    "    def loss_function(self,y,yt):\n",
    "        delta = 1e-7\n",
    "        loss = - np.sum(yt * np.log(y + delta))\n",
    "\n",
    "        return loss / len(yt)\n",
    "    \n",
    "\n",
    "    def accuracy(self,Z,Y):\n",
    "        return accuracy_score(Y,Z)\n",
    "\n",
    "    def fit(self, X, y, x_val = None, y_val= None):\n",
    "        for epoch in range(self.epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            self.loss = 0\n",
    "            for mini_x_train, mini_y_train in get_mini_batch:\n",
    "                # forward propagation\n",
    "                forward_data = mini_x_train\n",
    "                for layer in range(len(self.CNN)):\n",
    "                    forward_data = self.CNN[layer].forward(forward_data)\n",
    "                flt = Flatten()\n",
    "                forward_data = flt.forward(forward_data)\n",
    "                \n",
    "                \n",
    "                for layer in range(len(self.NN)):\n",
    "                    forward_data = self.NN[layer].forward(forward_data)\n",
    "\n",
    "                z = forward_data\n",
    "                \n",
    "                \n",
    "            backward_data = (z- mini_y_train)/ self.batch_size\n",
    "            layers = len(self.NN) - 1\n",
    "            for layer in range(len(self.NN)):\n",
    "                backward_data = self.NN[layers - layer].backward(backward_data)\n",
    "            \n",
    "            backward_data = flt.backward(backward_data)\n",
    "\n",
    "            for layer in range(len(self.CNN)):\n",
    "                backward_data = self.CNN[layer].backward(backward_data)\n",
    "            \n",
    "            self.loss += self.loss_function(z, mini_y_train)\n",
    "\n",
    "            self.log_loss[epoch] = self.loss / self.batch_size\n",
    "            self.log_acc[epoch] = self.accuracy(self.predict(X), np.argmax(y, axis=1))\n",
    "            if self.verbose:\n",
    "                print(self.loss/self.batch_size,self.accuracy(self.predict(X),np.argmax(y,axis=1)))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred_data = X[:, :, :]\n",
    "        for layer in range(len(self.CNN)):\n",
    "            pred_data = self.CNN[layer].forward(pred_data)\n",
    "            \n",
    "        pred_data = Flatten().forward(pred_data)\n",
    "        for layer in range(len(self.NN)):\n",
    "            pred_data = self.NN[layer].forward(pred_data)\n",
    "\n",
    "        return np.argmax(pred_data,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        n: Length of dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sigma = (2 / n) ** 0.5\n",
    "        \n",
    "    \n",
    "    def W(self, nodes):\n",
    "        return np.random.normal(0, self.sigma, nodes)\n",
    "    \n",
    "    def B(self, nodes):\n",
    "        return np.random.normal(0, self.sigma, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train.astype(np.float_), x_test.astype(np.float_)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "onv = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "one_hot_y_train = onv.fit_transform(y_train[:, np.newaxis])\n",
    "one_hot_y_test = onv.fit_transform(y_test[:, np.newaxis])\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, one_hot_y_train, test_size=0.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n",
      "C:\\Users\\Sylvanus\\AppData\\Local\\Temp\\ipykernel_25940\\1162421311.py:14: RuntimeWarning: invalid value encountered in log\n",
      "  loss = - np.sum(yt * np.log(y + delta))\n"
     ]
    }
   ],
   "source": [
    "NN = {\n",
    "    0:FC(784, 200, HeInitializer(784), AdaGrad(0.01), Relu()),\n",
    "    1:FC(200, 200, HeInitializer(200), AdaGrad(0.01), Relu()),\n",
    "    2:FC(200, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),\n",
    "}\n",
    "\n",
    "CNN = {\n",
    "    0: Conv2D(n_filter=10, n_channel_in=1, f_size=3, p=1, stride=1 ,initializer=SimpleInitializer(0.01),optimizer=SGD(0.01), activation = Relu())\n",
    "    #1:MaxPool2D(2),\n",
    "}\n",
    "\n",
    "model = ScratchConvolutionalNeuralNetwork2D(NN=NN, CNN=CNN).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x287ef9c7da0>]"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0905"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, model.predict(x_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
